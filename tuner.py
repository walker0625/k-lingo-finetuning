import os
import torch
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset

import subprocess
import sys

try:
    from dotenv import load_dotenv
except ImportError:
    print("ğŸ“¦ python-dotenvê°€ ì—†ìŠµë‹ˆë‹¤. ì„¤ì¹˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "python-dotenv"])
    from dotenv import load_dotenv

# 1. í™˜ê²½ ë³€ìˆ˜ ë° ê²½ë¡œ ì„¤ì •
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")
HF_REPO_ID = os.getenv("HF_REPO_ID")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(BASE_DIR, "data", "sample_level1.jsonl")
OUTPUT_DIR = os.path.join(BASE_DIR, "result", "adaptor")

# 2. ëª¨ë¸ ë¡œë“œ (RTX 4090 ìµœì í™”)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Qwen2.5-7B-Instruct-bnb-4bit",
    max_seq_length = 2048,
    dtype = None,
    load_in_4bit = True,
)

# 3. LoRA ì„¤ì •
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
)

# 4. í¬ë§·íŒ… í•¨ìˆ˜
def formatting_prompts_func(examples):
    texts = []
    for i in range(len(examples["input"])):
        text = (
            f"<|im_start|>system\n{examples['instruction'][i]}<|im_end|>\n"
            f"<|im_start|>user\n{examples['input'][i]}<|im_end|>\n"
            f"<|im_start|>thought\n{examples['thought'][i]}<|im_end|>\n"
            f"<|im_start|>call\n{examples['tool_call'][i]}<|im_end|>\n"
            f"<|im_start|>observation\n{examples['observation'][i]}<|im_end|>\n"
            f"<|im_start|>assistant\n{examples['output'][i]}<|im_end|>\n"
        )
        texts.append(text)
    return { "text" : texts }

# 5. ë°ì´í„° ì¤€ë¹„
dataset = load_dataset("json", data_files=DATA_PATH, split="train")
dataset = dataset.map(formatting_prompts_func, batched = True)

# 6. íŠ¸ë ˆì´ë„ˆ ì„¤ì • (RTX 4090ìš© bf16 í™œì„±í™”)
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = 2048,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60,
        learning_rate = 2e-4,
        bf16 = torch.cuda.is_bf16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        output_dir = "outputs",
    ),
)

# 7. ì‹¤í–‰ ë° ì €ì¥
trainer.train()
model.save_pretrained(OUTPUT_DIR)

# 8. í—ˆê¹…í˜ì´ìŠ¤ ì—…ë¡œë“œ
if HF_TOKEN and HF_REPO_ID:
    print(f"â˜ï¸ Uploading to Hugging Face: {HF_REPO_ID}")
    model.push_to_hub(HF_REPO_ID, token=HF_TOKEN)
    tokenizer.push_to_hub(HF_REPO_ID, token=HF_TOKEN)